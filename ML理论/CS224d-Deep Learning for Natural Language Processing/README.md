# [Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/syllabus.html)

## Lecture 1: Intro to NLP and Deep Learning
1.1 Linear Algebra Review
1.2 Probability Review
1.3 Convex Optimization Review
1.4 [More Optimization (SGD) Review](More Optimization (SGD) Review)
1.5 From Frequency to Meaning: Vector Space Models of Semantics
1.6 Lecture notes 1


## Lecture 2: Simple Word Vector representations: word2vec, GloVe
2.1 Distributed Representations of Words and Phrases and their Compositionality
2.2 Efficient Estimation of Word Representations in Vector Space
[`Assignment 1`](assignment1/index.html)

## Lecture 3: Advanced word vector representations: language models, softmax, single layer networks
3.1 GloVe-Global Vectors for Word Representation
3.2 Improving Word Representations via Global Context and Multiple Word Prototypes
3.3 Lecture notes 2

## Lecture 4: Neural Networks and backpropagation -- for named entity recognition
4.1 [UFLDL tutorial](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm)
4.2 Learning Representations by Backpropogating Errors
4.3 Lecture notes 3

## Lecture 5: Project Advice, Neural Networks and Back-Prop (in full gory detail)
5.1 Natural Language Processing (almost) from Scratch
5.2 A Neural Network for Factoid Question Answering over Paragraphs
5.3 Grounded Compositional Semantics for Finding and Describing Images with Sentences
5.4 Deep Visual-Semantic Alignments for Generating Image Descriptions
 
## Lecture 6: Practical tips: gradient checks, overfitting, regularization, activation functions, details
6.1 [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533)
6.2 [UFLDL page on gradient checking](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization)

## Lecture 7: Introduction to Tensorflow
7.1 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems
7.2 AWS Tutorial
7.3 [AWS Tutorial Supplementary](https://www.youtube.com/watch?v=zdnMXKHP-m4&feature=youtu.be)
[`Assignment 2`](http://cs224d.stanford.edu/assignment2/index.html)

# Lecture 8: Recurrent neural networks -- for language modeling and other tasks
8.1 Recurrent neural network based language model
8.2 Extensions of recurrent neural network language model
8.3 [Opinion Mining with Deep Recurrent Neural Networks](https://www.cs.cornell.edu/~oirsoy/drnt.htm)
8.4 [minimal net example (karpathy)](https://cs231n.github.io/neural-networks-case-study/)
8.5 [vanishing grad example](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)
8.6 [vanishing grad notebook](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.ipynb)
8.7 Lecture notes 4

## Lecture 9: GRUs and LSTMs -- for machine translation
9.1 Long Short-Term Memory
9.2 Gated Feedback Recurrent Neural Networks
9.3 Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
9.4 [proposal description](http://cs224d.stanford.edu/project.html#proposal)

## Lecture 10: Recursive neural networks -- for parsing
10.1 Parsing with Compositional Vector Grammars
10.2 Subgradient Methods for Structured Prediction
10.3 Parsing Natural Scenes and Natural Language with Recursive Neural Networks
10.4 Lecture notes 5

## Lecture 11: Recursive neural networks -- for different tasks (e.g. sentiment analysis)
11.1 Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
11.2 Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
11.3 Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks

## Lecture 12: Review Session for Midterm
12.1 [midterm solutions](http://cs224d.stanford.edu/midterm/midterm_solutions.pdf)

[`Assignment 3`](http://cs224d.stanford.edu/assignment3/index.html)

## Lecture 13: Convolutional neural networks -- for sentence classification
13.1 [A Convolutional Neural Network for Modelling Sentences](https://www.nal.ai/papers/Kalchbrenner_DCNN_ACL14)
13.2  [milestone description](http://cs224d.stanford.edu/project.html#milestone)

## Lecture 14: Guest Lecture with Andrew Maas: Speech recognition
14.1  Deep Neural Networks for Acoustic Modeling in Speech Recognition

## Lecture 15: Guest Lecture with Thang Luong: Machine Translation
15.1 Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models
15.2 Addressing the Rare Word Problem in Neural Machine Translation
15.3 Advances in natural language processing
15.4 Neural machine translation by jointly learning to align and translate

## Lecture 16: Guest Lecture with Quoc Le: Seq2Seq and Large Scale DL
- 16.1 Sequence to Sequence with Neural Networks
- 16.2 Neural Machine Translation by Jointly Learning to - Align and Translate
- 16.3 A Neural Conversation Model
- 16.4 Neural Programmer: Include Latent Programs with Gradient Descent

## Lecture 17: Neural Programmer: Include Latent Programs with Gradient Descent
- 17.1 [Ask me anthing: Dynamic Memory Networks for NLP](https://arxiv.org/abs/1506.07285)