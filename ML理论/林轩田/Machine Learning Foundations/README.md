# Machine Learning Foundations

## When can machines learn?
### Lecture 1	the learning problem:
0. Course Introduction
1. What is Machine Learning
2. Applications of Machine Learning
3. Components of Machine Learning
4. Machine Learning and Other Fields

### Lecture 2	learning to answer yes/no:
1. Perceptron Hypothesis Set
2. Perceptron Learning Algorithm (PLA)
3. Guarantee of PLA
4. Non-Separable Data

### Lecture 3	types of learning:
1. Learning with Different Output Space
2. Learning with Different Data Label
3. Learning with Different Protocol
4. Learning with Different Input Space

### Lecture 4	feasibility of learning:
1. Learning is Impossible?
2. Probability to the Rescue
3. Connection to Learning
4. Connection to Real Learning

---
## Why can machines learn?
### Lecture 5	training versus testing:
1. Recap and Preview
2. Effective Number of Lines
3. Effective Number of Hypotheses
4. Break Point

### Lecture 6	theory of generalization:
1. Restriction of Break Point
2. Bounding Function: Basic Cases
3. Bounding Function: Inductive Cases
4. A Pictorial Proof

### Lecture 7	the VC dimension:
1. Definition of VC Dimension
2. VC Dimension of Perceptrons
3. Physical Intuition of VC Dimension
4. Interpreting VC Dimension

### Lecture 8	noise and error:
1. Noise and Probabilistic Target
2. Error Measure
3. Algorithmic Error Measure
4. Weighted Classification

---
## How can machines learn?
### Lecture 9	linear regression:
1. Linear Regression Problem
2. Linear Regression Algorithm
3. Generalization Issue
4. Linear Regression for Binary Classification

### Lecture 10	logistic regression:
1. Logistic Regression Problem
2. Logistic Regression Error
3. Gradient of Logistic Regression Error
4. Gradient Descent

### Lecture 11	linear models for classification:
1. Linear Models for Binary Classification
2. Stochastic Gradient Descent
3. Multiclass via Logistic Regression
4. Multiclass via Binary Classification

### Lecture 12	nonlinear transformation:
1. Quadratic Hypotheses
2. Nonlinear Transform
3. Price of Nonlinear Transform
4. Structured Hypothesis Sets

---
## How can machines learn better?
### Lecture 13	hazard of overfitting:
1. What is Overfitting?
2. The Role of Noise and Data Size
3. Deterministic Noise
4. Dealing with Overfitting

### Lecture 14	regularization:
1. Regularized Hypothesis Set
2. Weight Decay Regularization
3. Regularization and VC Theory
4. General Regularizers

### Lecture 15	validation:
1. Model Selection Problem
2. Validation
3. Leave-One-Out Cross Validation
4. V-Fold Cross Validation

### Lecture 16	three learning principles:
1. Occam's Razor
2. Sampling Bias
3. Data Snooping
4. Power of Three
